## Things we will cover residency 2

A 50%-50% mix of SAS and other topics (Stata, statistical methods/issues, workflow/beautification).

## SAS procedures

We will cover:

- proc append: adding a dataset onto an existing dataset
- proc import: importing datasets
- proc logistic: regression where dependent variable is an indicator
- workflow: how to 'collect' output from many datasets (like the Hayn regression output).

## Stata

New things we will cover for Stata:

- Stata panel data, getting lagged (previous year), forward (next year) values easily
- Clustering of standard errors
- Collecting/organizing multiple regressions into a single table (to be beautified in Excel)

## 'Beautification'

Combined use of SAS, Stata, and Excel: exporting 'raw' numbers from SAS, Stata, and beautification in Excel (to be pasted into Word). The idea here is to minimize manual work when tables need to be updated.

## Propensity score matching

Using SAS we will go over propensity score matching.

We will run a logistic regression and find the drivers/factors that are correlated with having a restatement (Restatement = 1, a firm restated their financials, Restatement = 0, the firm did not). Propensity score matching is a procedure to get a control sample for some treatment sample. We will use this to find a control firm (no restatement) for each restatement firm such that both are equally likely to have a restatement.

## Difference-in-difference design

A difference-in-difference design is considered a very good (best?) setup:

- an exogenous shock (like new regulation), so that there is a 'before' and 'after'
- the shock doesn't affect all firms, so there are unaffected (control) and affected (treatment)

Example: in 2009, large firms (float of $700 mln or more) were required to file using XBRL. The first 'difference' is pre vs post (the firm as its own control), the second 'difference' is any difference of the pre vs post for other firms. 

Let's say you see that for large firms, the stock market has improved (more liquidity, smaller bid-ask spreads) (the first difference: pre vs post). Is that because of XBRL? Maybe, it depends if the same improvement is observed for the control group. (Could be that the increased liquidity is because of increase in algo trading.)

## Interaction variables 

How to use/interpret interaction variables. As an example (Stata), we will look at bargaining power of large firms (the larger the firm, the more bargaining power.) We will then interact with industry contract intensity.

Contract intensity is low when inputs are available on markets (like oil, poultry). Contract intensity is high when firms need to contract with other firms (aircraft manufacturing). 

## Endogeneity

We will discuss several sources of endogeneity, and how to deal with it (if possible). We will do an example for two stage least squares (Stata), and I'll provide an example of how to use the Heckman selection model (SAS and Stata).


## Other topics

- Making graphs using aggregates (median) on quintiles/deciles
- Dealing with outliers (Cook's distance, ranked regression)
- Multicollinearity (highly correlated variables)
- Scaling issues
- More on SAS macros 

## Breakdown by day (tentative)

### Friday

- Quick review
- How to collect relevant info out of many small datasets (table 4a Hayn) and beautify
- Macro correlation table
- Stata: panel data basics using sample for CI and size on competition and profitability
- Stata: multicolinearity, dealing with outliers: Cook’s distance, ranked regression 

### Saturday

- Creating graphs (median value by quintiles/deciles)
- Endogeneity, 2SLS instrumental variables 
- Difference-in-difference design
- Logistic regression (SAS) and propensity score matching (Restatement matched sample)


### Sunday

- Stata clustering
- Using Clay’s do_over and array macros
- Jackknife, sensitivity tests 
- Scaling issues
- Pseudo test

